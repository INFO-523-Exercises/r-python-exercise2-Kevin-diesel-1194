---
title: "Exercise 2"
author: "Vinu Kevin Diesel"
format: 
  pdf: 
    toc: true
    number-sections: true
    colorlinks: true
editor: visual
---

# **Installing required packages**

```{r}

options(repos = c(CRAN = "https://cloud.r-project.org"))

# First run this
install.packages("pacman")
```

```{r message=FALSE}
library(pacman)

p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr) 
```

# **Loading data**

### **CSV files (`.csv`)**

```{r}
data <- read_csv("data//x.csv")

data |> glimpse()

```

### **Tab separated values (`x.tsv`)**

```{r}
data <- read_delim("data//x.tsv")

data |> glimpse()
```

## **Importing data from MySQL database**

```{r}
drv <- dbDriver("MySQL") #obtain the driver for MySQL, drivers available for other DBMS
```

### **Using `dplyr` instead**

```{r}
install.packages("dbplyr") #install but don’t run library() on this dbplyr.
```

### **Obtain a connection**

```{r}
# Loading the required packages
library(DBI)
library(RMySQL)

# Defining the database connection parameters
db_user <- "termsuser"
db_password <- "Datamining"
db_name <- "etcsite_charaparser"
db_host <- "localhost"

# Creating a connection to the MySQL database
con <- dbConnect(MySQL(),
                 user = db_user,
                 password = db_password,
                 dbname = db_name,
                 host = db_host)

con

```

# **Data Cleaning**

## **Wide vs. long format**

```{r}
# Read data in wide format

wide <- read_delim("data//wide.txt", delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

```{r}
# The wide format uses the values (Math, English) of variable Subjects as variables.
# The long format should have Name, Subject, and Grade as variables (i.e., columns)
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

## **Long to wide, use `spread()`**

```{r}
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

## **Split a column into multiple columns**

```{r}
# Spliting Degree_Year to Degree and Year

clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean

```

## **Handling date/time and time zones**

```{r}
install.packages("lubridate")
library(lubridate)
```

```{r}
# Converting dates of variance formats into one format:

mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day format
clean.dates
```

```{r}
# Extracting day, week, month, year info from dates:

data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

```{r}
# Time zone:
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")

```

```{r}
# Converting to Phoenix, AZ time:

with_tz(date.time, tz="America/Phoenix")
```

```{r}
# Change the timezone for a time:

force_tz(date.time, "Turkey")
```

```{r}
# Check available time zones:

OlsonNames()

```

## **String Processing**

```{r}
# Libraries

library(dplyr)
library(stringr)
library(readr)

```

```{r}
# Fetching data from a URL, form the URL using string functions:

uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

```{r}
# str_c: string concatenation:

dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

```{r}
# Reading the data file:

data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

```{r}
#Dimentionality
dim(data)
```

```{r}
# Reading the name file line by line, put the lines in a vector:

lines <- read_lines(url(namesF))

lines |> head()
```

```{r}
# Examining the content of lines and see the column names start on line 67, ends on line 135. Then, get column name lines and clean up to get column names:

names <- lines[67:135]
names

```

```{r}
# Observing: a name line consists two parts, name: valid values. The part before : is the name.

names <- str_split_fixed(names, ":", 2) #split on regular expression pattern ":", this function returns a matrix
names

```

```{r}
# Taking the first column, which contains names:

names <- names[,1]
names
```

```{r}
# Now cleaning up the names: trim spaces, remove ():

names <-str_trim(names) |> str_replace_all("\\(|\\)", "") # we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.
names
```

```{r}
 # Finally, putting the columns to the data:
colnames(data)[1:69] <- names
data
```

```{r}
 # Renaming the last two columns:

colnames(data)[70:71] <- c("id", "class")
data
```

## **Dealing with unknown values**

```{r}
# Removing observations or columns with many NAs:
library(dplyr)

missing.value.rows <- data |>
  filter(!complete.cases(data))
missing.value.rows

```

```{r}
# How many NAs in each row? Applying a temporary function to the rows (“1”, if to columns use “2”) of data. This function counts the number of NAs in a row. If is.na(x) is TRUE (equivalent to 1), the sum of the booleans is then the count.

data <- data %>%
  mutate(na_count = rowSums(is.na(data)))
data
```

```{r}

```

```{r}
# Examining columns: how many NAs in each variable/column?

data |>
  summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

```{r}
# bser variable has 196 NAs. If this variable is considered not useful, given some domain knowledge, we can remove it from the data. From View, I can see bser is the 8th column:

data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
data.bser.removed
```

```{r}
# matches function can also help us find the index of a colname given its name:

data <- data %>%
  select(-matches("bser"))


```

### **Mistaken characters**

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

### **Filling unknowns with most frequent values**

```{r}
install.packages("DMwR2")
library(DMwR2)
data(algae, package = "DMwR2")
algae[48,]
```

```{r}
# mxPH is unknown. we will try to fill in with mean, median or something else

# plot a QQ plot of mxPH
install.packages("car")
library(car)
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

```{r}
# The straight line fits the data pretty well so mxPH is normal,let us try using mean to fill the unknown

algae <- algae |>
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

```{r}
#  About attribute Chla

ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

```{r}
median(algae$Chla, na.rm = TRUE)
```

```{r}
mean(algae$Chla, na.rm = TRUE)
```

```{r}
# From the above cells we can see that, the mean is not a representative value for Chla. For this we will use median to fill all missing values in this attribute, instead of doing it one value at a time:

algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))

```

### **Filling unknowns using linear regression**

```{r}
# This method is used when two variables are highly correlated. One value of variable A can be used to predict the value for variable B using the linear regression model.
```

```{r}
# Finding the variables in algae whihc are highly correlated:

algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric |> correlate() |> plot()
```

```{r}
cor_matrix
```

```{r}
# We can see from the matrix, PO4 and oPO4 are correct with a confidence level of 90%.

# Next, we will find the linear model between PO4 and oPO4:

algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. 

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

```{r}
# Checking the model goodness
m |> 
  summary()
```

```{r}
# or

m |> 
  summary() |> 
  tidy()

```

```{r}
# f a good model, coefficients should all be significant (reject Ho coefficience is 0), Adjusted R-squared close to 1 (0.8 is very good).

#F-statistics p-value should be less than the significant level (typically 0.05).

# While R-squared provides an estimate of the strength of the relationship between your model and the response variable, it does not provide a formal hypothesis test for this relationship.

# The F-test of overall significance determines whether this relationship is statistically significant.

# This model is good. We can also assess the fitness of the model with fitted line plot (should show the good fit), residual plot (should show residual being random).

# This lm is PO4 = 1.293*oPO4 + 42.897

algae$PO4
```

```{r}
# PO4 for observation 28 can then be filled with predicated value using the model

algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
```

```{r}
res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

```{r}
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

```{r}
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
#if x is not NA, return 42.897+1.293*x 
```

```{r}
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

### **Filling unknowns by exploring similarities among cases**

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

```{r}
# DM2R2 provides a method call knnImputation(). This method use the Euclidean distance to find the ten most similar cases of any water sample with some unknown value in a variable, and then use their values to fill in the unknown.

#We can simply calculate the median of the values of the ten nearest neighbors to fill in the gaps. In case of unknown nominal variables (which do not occur in our algae data set), we would use the most frequent value (the mode) among the neighbors. The second method uses a weighted average of the values of the neighbors.

# The weights decrease as the distance to the case of the neighbors increases.

algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


data(algae, package="DMwR2") #getting data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #using the median of k most similar samples
```

```{r}
getAnywhere(knnImputation())
```

# **Scaling and normalization**

```{r}
# Normalizing values in penguins dataset:

library(dplyr)
library(palmerpenguins)
```

```{r}
data(penguins)
```

```{r}
# selecing  only numeric columns
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# normalizing numeric columns
penguins_norm <- scale(penguins_numeric)

# converting back to data frame and adding species column
peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.
```

```{r}
summary(penguins)
```

```{r}
summary(peng.norm)
```

```{r}
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
```

```{r}
max
```

```{r}
min
```

```{r}
# min-max normalization
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

## **Discretizing variables (binning)**

```{r}
# Boston Housing data as an example:

data(Boston, package="MASS")
summary(Boston$age)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") #create 5 bins and add new column newAge to Boston
summary(Boston$newAge)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") #add labels

summary(Boston$newAge)
```

### **Equal-depth**

```{r}
install.packages("Hmisc")
library(Hmisc)
Boston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins and add new column newAge to Boston

table(Boston$newAge)
```

### **Assign labels**

```{r}
Boston$newAge <- factor(cut2(Boston$age, g = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

```{r}
# Plot an equal-width histogram of width 10:

hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 – 101.
```

```{r}
# Or using ggplot2

library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

## **Decimal scaling**

```{r}
data <- c(10, 20, 30, 50, 100)
```

```{r}
(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters
```

```{r}
(decimalScale = data / (10^nDigits))
```

### **Smoothing by bin mean**

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

```{r}
(bin_means = apply(bins, 1, FUN = mean))
```

```{r}
# Finding the average of each bin:

(bin_means = apply(bins, 1, FUN = mean))
```

```{r}
# Replacing values with their bin mean:

for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

# **Variable correlations and dimensionality reduction**

## **Chi-squared test**

```{r}
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

## **Loglinear model**

```{r}
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
# Observing how data is saved in 2X2X2X2 Array
seniors
```

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

```{r}
# Using * to connect all variables to get a saturated model, which will fit the data perfectly. Then removing effects that are not significant.

mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

```{r}
mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

```{r}
cbind(mod.3$data, fitted(mod.3))
```

## **Correlations**

```{r}
library(tidyr) # data manipulation
penguins_numeric |> 
  drop_na() |>
  correlate()
```

## **Principal components analysis (PCA)**

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
head(pca$scores) # pca result is a list, and the component scores are elements in the list
```

```{r}
penguins_na <- penguins |> 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

```{r}
install.packages("wavelets")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3) #with 8-element vector, 3 level is the max.
wt
```

```{r}
# Reconstruct the original:

idwt(wt)
```

```{r}
# Obtaining transform results as shown in class, using a different filter:

xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

```{r}
# Reconstructing the original:

idwt(xt)
```

# **Sampling**

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

## **Simple random sampling, without replacement:**

```{r}
sample(age, 5)
```

## **Simple random sampling, with replacement:**

```{r}
sample(age, 5, replace = TRUE)
```

## **Stratified sampling**

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

## **Cluster sampling**

```{r}
library(sampling)
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

# **Handling Text Datasets**

```{r}
pacman::p_load(tm,SnowballC)#tm uses SnowballC for stemming
# read corpus
# Emails.csv,  holding some of Hillary's emails
data <- read.csv("data//Emails.csv", stringsAsFactors = FALSE)

docs <- Corpus(VectorSource(data$RawText))
mode(docs)
```

## **Inspect a document**

```{r}
docs[[20]]
```

## **Preprocessing text**

```{r}
docs <- docs |>
         tm_map(removePunctuation) |>
         tm_map(content_transformer(tolower)) |> #to lower case
         tm_map(removeNumbers) |>
         tm_map(removeWords, stopwords("en")) |> #stopwords, such as a, an.
         tm_map(stripWhitespace) |>
         tm_map(stemDocument) #e.g. computer -> comput
```

```{r}
content(docs[[20]]) #note: stemming reduces a word to its ‘root’ with the aassumption that the ‘root’ represents the semantics of a word, e.g. computer, computing, computation, computers are about the concept of compute, which may be represented by ‘comput’. but stemming is never perfect.
```

```{r}
# Converting text to a matrix using TF*IDF scores (see TF*IDF scores in Han’s text):

DTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
DTData
```

```{r}
inspect(DTData[1:2, 1:5])
```

```{r}
# Creating term-document matrix (also called inverted index, see Han’s text in a later chapter):

TDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
inspect(TDData[1:2, 1:5])
```

## **Explore the dataset**

```{r}
findFreqTerms(TDData, lowfreq = 75, highfreq = 1000)
```

```{r}
# Finding correlations among terms:

findAssocs(TDData, terms = "bill", corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("bill"), corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("schedul"), corlimit = 0.3)
```

## **Create a word cloud**

```{r}
install.packages("wordcloud")
install.packages("RColorBrewer")
```

```{r}
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

```{r}
# png() opens a new device ‘png’ to output the graph to a local file:
library(wordcloud)

png(file = "wordCloud.png", width = 1000, height = 700, bg= "grey30")

wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

```{r}
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

```{r}
wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

```{r}
install.packages("onehot")
library(onehot)
d <- data.frame(language=c("javascript", "python", "java"), hours=c(10, 3, 5) )
d$language <- as.factor(d$language) #convert the column to be encoded to Factor
encoded <- onehot(d)
new_d <- predict(encoded, d)
new_d
```

```{r}
install.packages("qdapTools")
library(qdapTools)
d <- data.frame(language=c("javascript, python", "java"), hours = c(3, 5) )
d
```

```{r}
dlist <- as.list(d)
new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

new_d
```

```         
```
